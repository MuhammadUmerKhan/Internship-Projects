import os, logging  # ğŸ“¥ Import os for file path operations and logging for logging functionality
from langchain_groq import ChatGroq  # ğŸ“¥ Import ChatGroq for interacting with Groq API's LLM
from config import config as CONFIG  # ğŸ“¥ Import config module (as CONFIG) for accessing API keys and settings

os.makedirs(os.path.join("logs"), exist_ok=True)  # ğŸ“‚ Creates a 'logs' directory if it doesn't exist
logging.basicConfig(  # âš™ï¸ Configures the logging system with specified settings
    level=logging.INFO,  # ğŸ“ Sets logging level to INFO for informational messages and above
    format="%(asctime)s [%(levelname)s] %(message)s",  # ğŸ“ Defines log message format: timestamp, level, and message
    handlers=[  # ğŸ“¤ Specifies where logs are sent
        logging.FileHandler(os.path.join("logs", "logging.log")),  # ğŸ“œ Logs to a file named 'logging.log' in the 'logs' directory
        logging.StreamHandler()  # ğŸ–¥ï¸ Also logs to the console (standard output)
    ]
)

def configure_llm(MODEL_NAME):  # ğŸ¤– Function to configure the language model (LLM)
    """
    Configure LLM to run on Hugging Face Inference API (Cloud-Based).
    
    Args:
        MODEL_NAME (str): Name of the model to configure (e.g., "qwen-qwq-32b").
    
    Returns:
        llm (LangChain LLM object): Configured model instance, or error message if configuration fails.
    """
    try:
        # logging.info(f"ğŸ¤– Querying LLM: {MODEL_NAME}")  # ğŸ“ (Commented) Log the model being queried
        llm = ChatGroq(  # ğŸŒ Initializes the ChatGroq model for cloud-based inference
            temperature=0,  # ğŸŒ¡ï¸ Sets temperature to 0 for deterministic responses
            groq_api_key=CONFIG.GROQ_API_KEY,  # ğŸ”‘ Uses the Groq API key from config
            model_name=MODEL_NAME  # ğŸ“› Specifies the model name (e.g., "qwen-qwq-32b")
        )
        logging.info(f"âœ… {str(MODEL_NAME).capitalize()} Loaded!")  # âœ… Logs success of model loading
        return llm  # ğŸ”„ Returns the configured LLM instance
    except Exception as e:  # âš ï¸ Catches any errors during LLM configuration
        logging.error(f"âŒ LLM Query Error: {str(e)}")  # âŒ Logs the error details
        return "âŒ Error generating LLM response."  # ğŸš« Returns an error message if configuration fails